---
header-includes: \usepackage{color}
                 \usepackage{float}
output:
  html_document: default
  pdf_document:
    fig_caption: no
---

```{r, echo=FALSE, warning=FALSE, message=FALSE}
source("../R/setup.rmd.R", local=TRUE)
setup.rmd(local.env=environment())
```
`r hl()$basefontsize()`
`r hl()$style()`

## Model Notation

A number of R routines (for example boxplot and lm) use the *model* notation *y~x*, which you should read as *y modeled as a function of x*. So for example if we want to find the least squares regression model of y on x we use

```{r eval=FALSE}
lm(y ~ x)
```

In standard math notation that means fitting an equation of the form

$$
Y = \beta_0 + \beta_1 x + \epsilon
$$
Sometimes one wants to fit a no-intercept model:

$$
Y = \beta_1 x + \epsilon
$$
and this is done with

```{r eval=FALSE}
lm(y ~ x - 1)
```

If there are two predictors you can 

-  fit an *additive* model of the form
$$
Y = \beta_0 + \beta_1 x + \beta_2 z + \epsilon
$$

with

```{r eval=FALSE}
lm(y ~ x + z)
```

-  fit a model with an *interaction* term 

$$
Y = \beta_0 + \beta_1 x + \beta_2 z + \beta_3 x \times z + \epsilon
$$

with

```{r eval=FALSE}
lm(y ~ x * z)
```

`r hl()$vspace()`

In the case of three (or more predictors) there are all sorts of possibilities:

-  model without interactions

$$
Y_i = \beta_0 + \sum_{i=1}^n \beta_i x_i + \epsilon
$$

```{r eval=FALSE} 
lm(y ~ x1 + x2 + x3)
```

-  model with all interactions

$$
Y_i = \beta_0 + \sum_{i=1}^n \beta_i x_i +
\sum_{i,j=1}^n \beta_{ij} x_i x_j + \\
+ \text{ ... } + \\
\beta_{1..n}x_1 \times .. \times x_n + \epsilon
$$

```{r eval=FALSE} 
lm(y ~ (x1 + x2 + x3)^3 )
```

-  model with all pairwise interactions

$$
Y_i = \beta_0 + \sum_{i=1}^n \beta_i x_i + 
\sum_{i,j=1}^n \beta_{ij} x_i x_j  + \epsilon
$$

```{r eval=FALSE} 
lm(y ~ (x1 + x2 + x3)^2 )
```

these model descriptions are not unique, for example the last one is equivalent to

```{r eval=FALSE} 
lm(y ~ x1 * x2 * x3 - x1:x2:x3)
```

Sometime we want * to indicate actual multiplication and not interaction. This can be done with

```{r eval=FALSE} 
lm(y ~ x1 + x2 + I(x1*x2))
```

Another useful one is ., which stands for *all +'s*, so say (y, x1, x2, x3) are the columns of a dataframe df, then

```{r eval=FALSE} 
lm(y ~ x1 + x2 + x3, data=df )
```

is the same as

```{r eval=FALSE} 
lm(y ~ ., data=df )
```

and

```{r eval=FALSE} 
lm(y ~ .*x3, data=df )
```

is the same as

```{r eval=FALSE} 
lm(y ~ x1 + x2 + x3 + x1*x3 +x2*x3)
```

`r hl()$vspace()`

if there are more than a few predictors it is usually easier to generate a matrix of predictors:

```{r eval=FALSE}
X <- cbind(x1, x2, x3, x4)
lm(y ~ X)
```


#### **Case Study**: House Prices

we have a list of prices and other information on houses in Albuquerque, New Mexico:

```{r}
head(albuquerquehouseprice)
```

-  additive model, all four predictors:

```{r}
summary(lm(Price ~ Sqfeet + 
             Feature + Corner + Tax,
           data=albuquerquehouseprice))
```

- additive model, Sqfeet and Features

```{r}
summary(lm(Price ~ Sqfeet + Feature,
           data=albuquerquehouseprice))
```

- model with interaction, Sqfeet and Features

```{r}
summary(lm(Price ~ Sqfeet * Feature,
           data=albuquerquehouseprice))
```

- model with pairwise interactions:

```{r}
summary(lm(Price ~ (Sqfeet + Feature +
                      Corner +  Tax)^2,
           data=albuquerquehouseprice))
```

- model with all possible terms:

```{r}
summary(lm(Price ~ (Sqfeet + Feature +
                      Corner +  Tax)^4,
           data=albuquerquehouseprice))
```


