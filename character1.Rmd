---
header-includes: \usepackage{color}
output:
  html_document: default
  pdf_document:
    fig_caption: no
---

<style>
table, th, td { text-align:right; }
th, td {padding: 10px;}
</style>

```{r, echo=FALSE, warning=FALSE, message=FALSE}
whichcomp <- strsplit(getwd(),"/")[[1]][3]
load(paste0("c:/users/", whichcomp, "/Dropbox/teaching/Resma3/Resma3.RData"))
library(knitr)
opts_chunk$set(fig.width=6, fig.align = "center", 
      out.width = "70%", warning=FALSE, message=FALSE)
library(ggplot2)
library(grid)
```

`r hl()$basefontsize()`

#Character Manipulation with stringr

```{r}
library(magrittr)
library(dplyr)
```


Thanks to Hadley Wickham, we have the package *stringr* that adds more functionality to the base functions for handling strings in
R. According to the description of the package at [http://cran.r-project.org/web/packages/stringr/index.html
](http://cran.r-project.org/web/packages/stringr/index.html) stringr
is a set of simple wrappers that make R’s string functions more consistent,  simpler and easier to use.  It does this by ensuring that:  function and argument names (and positions)  are  consistent,  all  functions  deal  with  NA’s  and  zero  length  character  appropriately, and the output data structures from each function matches the input data structures of other functions. 

We previously looked at the states in the US:

```{r, echo=2}
states <- agesexUS$State
head(states)
```

We can find out how many letters each state has with

```{r}
library(stringr)
states %>%
  str_length()
```

Also, we found out how many vowels the names of each state had. Here is how we can do that with *stringr*:

```{r}
states %>%
  str_count("a")
```

Notice that we are only getting the number of a’s in lower case.  Since str_count() does not contain the argument ignore.case, we need to transform all letters to lower case, and then count the number of
a’s like this:

```{r}
states %>%
  tolower() %>%
  str_count("a")
```

Now let's do this for all the vowels:

```{r error=T}
vowels <- c("a", "e", "i", "o", "u")
states %>% 
  tolower() %>% 
  str_split("") %>% 
  unlist() %>%
  table() ->
  x
x[vowels]
```

`r hl()$hr()`

*stringr* provides  functions  for  both  

1)  basic  manipulations  
2)  regular  expression operations

The following table contains the *stringr* functions for basic string operations: 


|Function | Description | Similar to |
|---|---|---|
|str_c()  | string concatenation | paste()
|str_length() | number of characters | nchar() |
|str_sub() | extracts substrings | substring()|
|str_dup() | duplicates characters | none|
|str_trim() | removes leading and trailing whitespace | none|
|str_pad() | pads a string | none |
|str_wrap() | wraps a string paragraph | strwrap() |
|str_trim() | trims a string | none |

Here are some examples:

```{r}
paste("It", "is",  "a", "nice", "day", "today")
str_c("It", "is",  "a", "nice", "day", "today")
str_c("It", "is",  "a", "nice", "day", "today",
      sep=" ")
str_c("It", "is",  "a", "nice", "day", "today",
      sep="-")
```

next str_length. Compared to nchar() it can handle more data types, for example factors:

```{r}
some_factor <- factor(c(1, 1, 1, 2, 2, 2), 
          labels = c("good", "bad"))
some_factor
str_length(some_factor)
```

whereas nchar(some_factor) results in an error.

A routine that has no direct equivalent in basic R is str_dup. It is sort of a rep for strings:

```{r}
str_dup("ab", 2)
str_dup("ab", 1:3)
```

Another handy function that we can find in stringr is str_pad() for padding a string. This is useful if we want to have a nice alignment when printing some text.

Its default usage has the following form:

str_pad(string, width, side = "left", pad = " ")

The  idea  of str_pad() is  to  take  a  string  and  pad  it  with  leading  or  trailing  characters to  a  specified  total width. The  default  padding  character  is  a  space  (pad = " "),  and consequently the returned string will appear to be either left-aligned (side = "left"
), right-aligned (side = "right"), or both (side = "both").

Let’s see some examples:

```{r}
str_pad("Great!", width = 7)
str_pad("Great", width = 8, side = "both")
str_pad("Great!", width = 7, pad="#")
```

Often when dealing with character vectors we end up with white spaces. These are easily taken care of with str_trim:

```{r}
txt <- c("some", " text", "with ", " white ", "space")
str_trim(txt)
```

An operation that one needs to do quite often is to extract the last (few) letters from words. Using substring is tricky if the words don't have the same lengths:

```{r}
substring(txt, nchar(txt)-1, nchar(txt))
```

Much easier with

```{r}
str_sub(txt, -2, -1)
```

You can use str_wrap() to modify existing whitespace in order to wrap a paragraph of text, such that the length of each line is as similar as possible.

```{r}
declaration.of.independence <- "We hold these truths to be self-evident, that all men are created equal, that they are endowed by their Creator with certain unalienable Rights, that among these are Life, Liberty and the pursuit of Happiness."
cat(str_wrap(declaration.of.independence, width=40))
```

##Dracula by Bram Stoker

Let's do a textual analysis of Bram Stoker's Dracula. We can get an electronic copy of the book from the Project Gutenberg [http://www.gutenberg.org/](http://www.gutenberg.org/). To get a book into R is very easy, there is a package:

```{r cache=TRUE}
library(gutenbergr)
dracula <- gutenberg_download(345) 
dracula
```

Why 345? This is the id number used by the Gutenberg web site to identify this book. Go to their website and check out what other books they have (there are over 57000 as of 2018).

The first column is the gutenber_id, so we can get rid of that

```{r}
dracula <- dracula[, 2]
```

Let's see the beginning of the book:

```{r}
dracula[1:100, ] %>% 
  str_wrap(width=40) %>% 
  cat()
```

What are the most commonly used words in the book? Well, it will be something like "a", "and" etc. Those kinds of words are called *stop_words*, and they are not very interesting, and it might be better to just take them out. There are lists of such words. One of them is in the liberary *tidytext*:

```{r}
library(tidytext)
stop_words
```

So we now want to go through Dracula and remove all the appearances of any of the words in stop_words. This can be done with the *dplyr* command *anti_join*. However the two lists need to have the same column names, and in Dracula it is *text* whereas in stop_words it is *word*. Again, the library *tidytext* has a command:

```{r}
dracula %>% 
  unnest_tokens(word, text) %>% 
  anti_join(stop_words) ->
  dracula
dracula
```

So now for the most common words:

```{r}
dracula %>% 
  count(word, sort=TRUE)
```

so *time* is the most common word, it appears 390 times in the book.

How do the words in Dracula compare to another famous fiction book of the era, The Time Machine, by H. G. Wells? This is book #35 in the Gutenberg catalogue:

```{r cache=TRUE}
time.machine <- gutenberg_download(35)[, 2]
time.machine %>% 
  unnest_tokens(word, text) %>% 
  anti_join(stop_words) ->
  time.machine
```

```{r}
time.machine %>% 
  count(word, sort=T)
```

Actually, time is the most common word in both books (not a surprise in a book called The Time Machine!)

Can we do a graphical display of the word frequencies? We will need some routines from yet another package called *tidyr*.

We begin by joining the two books together, with a new column identifying the book:

```{r}
library(tidyr)
freqs <- bind_rows(mutate(dracula, book="Dracula"),
       mutate(time.machine, book="Time Machine"))
freqs
```

Next we add some useful columns:

```{r }
freqs %>% 
  mutate(word=str_extract(word, "[a-z']+")) %>% 
        #take out things like , etc
  count(book, word) %>% 
  group_by(book) %>%
  mutate(prop=n/sum(n)) %>% #take into account 
                  #different lengths of the books
  ungroup() %>% 
  filter(n>10) %>%  #consider only words used frequently
  select(-n) %>%  #not needed anymore
  arrange(desc(prop)) ->
  freqs
```

Next we find all the words that appear in both books, and look at their relative proportions:

```{r}
freqs %>% 
   spread(key=book, value = prop) %>% 
            #use one column for Dracula's 
            #proportions and another for Time Machine
   na.omit()  -> #words that appear in only one book are NA, 
                #eliminate them    
  common.words
print(common.words, n=4)
common.words %>%
  ggplot(aes(x=Dracula, y= common.words[ ,"Time Machine"])) +
    labs(x = "Dracula", 
         y = "Time Machine") +
    geom_text(aes(label = word), 
              check_overlap = TRUE, 
              vjust = 1.5)
```
